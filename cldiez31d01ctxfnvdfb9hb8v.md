# Models for the mental...

So, first of all, a couple of mental models from a [nice compilation](https://readwise.io/reader/shared/01gm16e6g8423mr7njrzdp1f1p/):

### Littlewood's law

*We can expect ‚Äúmiracles‚Äù to happen regularly, because, in a world with 7 billion people, the odds of a one-in-a-billion event are pretty good.*

It's, by Freeman Dyson's explanation and with *1/1,000,000* miracle chance, we'll have *8\*3,600\*30 = 864,000* perceivable events per month, bringing the "miracle probability vessel" to almost fullness. As for me, it's just the incomprehensibility of the sheer interconnectedness of our lives and events and has less to do with probability and the gambler's fallacy.

![Gambler's Fallacy And Why It Matters In Business - FourWeekMBA](https://i0.wp.com/fourweekmba.com/wp-content/uploads/2020/11/gamblers-fallacy.png?fit=2560%2C1931&ssl=1 align="center")

### Gibson‚Äôs law

*‚ÄúFor every PhD there is an equal and opposite PhD.‚Äù* In law and public policy, the observation is that equally qualified expert witnesses can come to opposite conclusions.

One good example springing to my mind was a [recent paper](https://twitter.com/ZakharKogan/status/1581909360942649345) showing that 73 teams all made different conclusions from the same data:

![Image](https://pbs.twimg.com/media/FfEDh5paAAAxedz?format=jpg&name=large align="left")

### **Brandolini‚Äôs law**

*‚ÄúThe amount of energy needed to refute bullsh\*t is an order of magnitude bigger than to produce it.‚Äù*

Coined by Italian software developer [Albert Brandolini](https://twitter.com/ziobrando/status/289635060758507521), who also refers to it as the **Bullsh\*t Asymmetry Principle**.

![Jos√© Gonz√°lez ü•ë on Twitter: "@katieboue In case a burrito dog helps...  https://t.co/8nk71eUBFv" / Twitter](https://pbs.twimg.com/media/DgAXLuZX0AEciyd.jpg align="left")

One of the ways to mask bullshit is, usually, **wrapping it into layers of complexity** as the [attribution substitution](https://en.wikipedia.org/wiki/Attribute_substitution) says. Another is **putting the bullshit part into notable and prominent conclusions** as [salience bias](https://en.wikipedia.org/wiki/Salience_(neuroscience)#Salience_bias) calls for.

### Goodhart's law

*When a measure becomes a target, it stops being a good measure, or once a goal is set, people will optimize for that goal in a way that neglects equally important parts of a system*. In other words, a case of [unintended consequences](https://en.wikipedia.org/wiki/Unintended_consequences).

Apart from the usual [cobra example](https://en.wikipedia.org/wiki/Perverse_incentive), we can touch the [slave redemption](https://mru.org/courses/principles-economics-microeconomics/elasticity-example-slave-redemption-sudan):

![Elasticity of Supply ‚Äì Atlas of Public Management](http://www.atlas101.ca/pm/wp-content/uploads/2016/04/SlaveRedemption2.jpg align="center")

*Not that helpful as we're just breeding more slavery cases. Some thoughts resemble the* [*SIS infection model*](https://en.wikipedia.org/wiki/Compartmental_models_in_epidemiology#The_SIS_model)*, only with S = Slave, as people freed are again susceptible to being taken a slave for buyback.*

### Metcalfe's law [turned multivariate](https://www.linkedin.com/pulse/metcalfes-law-network-effects-strategic-partner-chad-barker/)

Metcalfe's law on network's value being *nodeNumber^2* may be a bit outdated: we're starting to live in an increasingly complex world, and even pizza is now too complex and uncertain to comprehend - that's why we just eat it to manage the entropy.

The multivariate M's law, or **Conrad's corollary** as per the author, states that the value of a network depends, too, on:

* Number of nodes (Metcalfe's original point)
    
* Value of each node
    
* Strength of connection between the nodes (mentioned in [NFX's Network MC](https://www.nfx.com/masterclass/network-effects/network-bonding-theory))
    
* Network arrangement
    
* Network purpose
    

Another statement is we should aim for [non-linear, explosive growth](https://www.nfx.com/masterclass/network-effects/network-theory-properties):

![](https://media.licdn.com/dms/image/C5612AQECPFPTfi13LA/article-inline_image-shrink_1000_1488/0/1523467154349?e=1680134400&v=beta&t=_obj-x1N5qfcQRttf-smJB7RQMP8FqUDz6U1SBNcTnw align="center")

How can we achieve that? The **magic of compounding** - it can become a coffin nail as in the case of FTX/Alameda, or a huge boon in the case of a small MVP startup, carefully engineered.

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1674927361757/61d45607-fff0-45db-b827-2d34c8d7f34f.png align="center")

*Pretty obvious, huh?*

### Dollo's law

In evolution, organisms can‚Äôt re-evolve to a former state because the path that led to its former state was so complicated that the odds of retracing that exact path round to zero.

Another thought: **re-creating a totally-human AGI could require simulating the whole evolutionary process**, or simulating something in its entirety requires **simulating it from first principles/down to atom interactions**. Hence simulating a brain's region [took so much compu-power](https://www.popsci.com/article/science/supercomputer-takes-40-minutes-create-super-detailed-model-1-second-brain-activity/).

Yet, **in 2013 it took 40 minutes to simulate a second of 1% activity** - let's hope we move forward and see more whole-brain emulators in the coming years. Plus, an estimated &gt;50% of the brain cortex is dedicated to visual processing - we won't need that. <s>And if we consider how much is dedicated to anxiety and thoughts...</s>

![Researchers Find Algorithm for Large-Scale Brain Simulations - Research &  Development World](https://media-rd.s3.amazonaws.com/embedded_image/2018/03/breaintech.jpg align="left")

In the next issue (I got lazy):

* Parkinson's law
    
* Wiio's laws
    
* Sayre's law
    
* Stigler's law
    
* Mill mistake
    
* Hickam's dictum
    

## Links for the un-lazy:

* A couple of articles from Indie Hackers I've highlighted:
    
    * [Saasify VC Feedback](https://readwise.io/reader/shared/01gm456c7gfkp1ky0c0egve3kj)
        
    * An extremely good fundraising write-up: [How I Raised VC From Top-Tier Funds](https://readwise.io/reader/shared/01gm44gm2jpzyd0zv4tvhtnr2z)
        
    * [The riches are in the (data) niches](https://readwise.io/reader/shared/01gm5w0whk8bsthag7y2hdjck9): historical data, valuable data, data prediction/speculation, niche data, new/relevant data etc.
        
    * Why [lifetime deals are detrimental](https://readwise.io/reader/shared/01gkza3xav9m5yermq52xkehm1) to your LTV in the long run
        
    * [15 pieces of advice for beginning bootstrapped SaaS founders](https://readwise.io/reader/shared/01gmfhq00ssfrc3ntda2dbaw89): build small and focus, automate and track, focus on customer service and 'being there for \_', don't sniff coke until first 1M ARR.
        
    * [30 actionable ideas proven to jumpstart growth](https://readwise.io/reader/shared/01gkza3vbxkshsp79jwc09x74e): giveaways, partnerships, limited offers, onboarding, freemium etc.
        
* Yet another "Huh an AI" [compilation](https://theresanaiforthat.com/), and [another one](https://twitter.com/DCLBlogger/status/1599684210322591747)
    

---

Welcome to **YAWN/Boi Diaries**‚ù£Ô∏è You can find the other blogs I try to cross-post to:

* [**Hashnode**](https://yawn.hashnode.dev/)
    
* [**Medium**](https://baldr.medium.com/)
    
* [**Telegram**](https://t.me/ohmyboi)
    
* [**Twitter**](https://twitter.com/ZakharKogan) (for a lot of stuff)
    
* Maybe even a [**LinkedIn**](https://www.linkedin.com/in/zakhar-kogan/)?