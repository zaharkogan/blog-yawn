# So causal, much brain

Aand again we're on AI, 'cause I'm not reading enough biochemistry. Next after that - mental models pt. 2 ‚ù£Ô∏è

Recently we've discussed that neural networks do usually [understand shiet](https://hashnode.com/post/cle14dmka00nwaunv7x5nhqqx) ‚á¢ that got me interested in how can they be made to understand causal relationships?

## CausalKG

One approach is definitely causal reasoning. I stumbled upon a [paper](https://www.researchgate.net/publication/357765711_CausalKG_Causal_Knowledge_Graph_Explainability_using_interventional_and_counterfactual_reasoning) outlining a possible structure of a causal knowledge graph. Okay, too close to [dependent origination](https://en.wikipedia.org/wiki/Prat%C4%ABtyasamutp%C4%81da) from Buddhism, but we'll manage.

![illustration 12 links of dependent origination - Google Search ...](https://i.pinimg.com/originals/5c/ca/db/5ccadb2e0615fd4c3d7e6d5fdb03ab58.png align="center")

*Or not*

The authors propose the following structure:

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1677661872202/692e2ed0-5bae-4c43-99de-e47bd494f9a2.png align="center")

*\[subject ‚áí causes {mediator: , direct:, indirect:} ‚áí object\]*

Which combines a Bayesian causal network *(what causes what)* and an [ontology network](https://en.wikipedia.org/wiki/Ontology_(computer_science)#Domain_ontology) *(what is X in some domain, and relation types)*.

Or, as BinGPT eloquently presents it, *"Ontology network for causal learning is a way of using existing knowledge to find out what causes what in a domain."*

In the end, it comes to a graph like that:

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1677665553661/06b65e90-897a-4e72-8918-1bad4fc47f31.png align="center")

Another visualization can be found [here](https://goodboychan.github.io/machine_learning/2020/10/05/01-Causal-Graphical-Model.html).

## CausalUZ<s>bekistan</s>

As I'm currently engaged in a startup building an associative database (later **AMD, Associative Model of Data**) based on [S. William's findings](https://link.springer.com/content/pdf/10.1057/palgrave.jdm.3240049.pdf), some of the thoughts crossing my mind are *"how the f\*ck do we combine associative data and causal reasoning?"*

As AMD deals with two atomic data types: **items** and **links**, and the startup + its [underlying ideas](https://github.com/linksplatform) use a modified and unified data structure - a **link:** <mark>[id, type_id, from*, to*]</mark>. So, for a node, we just omit **from** and **to** fields. A not-so-brief introduction was made by me [here](https://docs.google.com/presentation/d/1R03_yJeoLBypkqln36FmWH8A0NiXkMO_ujmgQlzPRT4/edit?usp=sharing) on slides 14-24.

![Comparison of models](https://github.com/LinksPlatform/Documentation/raw/master/doc/ModelsComparison/relational_model_vs_associative_model_vs_links.png align="center")

So, we can basically express a *subject ‚áí predicate ‚áí object* relationship. Can we do better, expressing a causal relationship?

I've thought to combine the strengths of the modified AMD <s>Ryzen</s> with knowledge network construction, expanding it to a semi-fixed set of predicates. "Causes" is great, but LLMs can derive what more complex things mean and when they are used.

I've thought of the following structure:

![](https://latex2image-output.s3.amazonaws.com/img-jBPchuSQNH61.png align="left")

So we could use an agreed or expanding set of predicates/causal relationships. The **link** unit structure will allow pointing (as it's essentially a pointer) the mediator or other modifiers (say, a bias) to another edge:

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1677668423573/fc8f8904-fcc4-4cef-b63f-44e9a0d2e5c8.jpeg align="center")

### Field tests (no)

I've tried this with obvious ChatGPT¬†with mixed success:

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1677668482086/d02a9c70-1652-4f31-bb53-af0a94a83329.png align="center")

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1677668502605/ba700946-4b72-4e02-b9fa-8327199a0ecc.png align="center")

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1677668518792/90117959-8200-4003-99f9-4b769b2e35df.png align="center")

[Anthropic's Claude](https://poe.com/) seems to fare a bit better:

`Based on the causal graphs provided, it is unlikely that a mouse can found a startup on mousetraps. While the mouse has expertise, ability and initiative which could be useful for a startup founder, the mouse is more likely to use those capabilities to evade mousetraps than to found a startup focused on mousetraps. The mousetraps -> startup edge suggests that the idea, profitability and good market for mousetraps could lead to a startup, but the mouse -> mousetraps edge suggests the mouse is more likely to avoid mousetraps than build a business around them. So, while possible, it is unlikely based on the provided causal knowledge.`

## Conclusion

![](https://miro.medium.com/max/1400/1*QzaC-G_SMsJd2AJNyIv8PQ.png align="center")

[*Source*](https://medium.com/@ToferC/network-graph-of-canadian-legislation-8c6228bd3d0d)

All in all, I believe that the combination of LLMs and the updated causal structure I've provided can help <s>make the world better</s> the networks understand the world a wee bit better, leading to less BS, more usefulness ü§î Would love feedback on that <s>plz teach to upload to Arxiv</s>.